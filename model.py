# -*- coding: utf-8 -*-
"""Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xOvR37Lc9-sDmn6C8WfEATgVKh5umJkb
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install datasets
# %pip install transformers

from datasets import load_dataset

dataset = load_dataset("cnn_dailymail", '3.0.0')
print(f"Features: {dataset['train'].column_names}")

multi = load_dataset('multi_news')
print(multi)

dataset

sample = dataset["train"][1]
print(f"""Document (excerpt of 2000 characters, total length: {len(sample["article"])}):""")
print(sample["highlights"][:2000])
print(f'\nSummary (length: {len(sample["highlights"])}):')
print(sample["article"])

from transformers import BartForConditionalGeneration, AutoTokenizer

model_ckpt = "sshleifer/distilbart-cnn-6-6"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = BartForConditionalGeneration.from_pretrained(model_ckpt)

def convert_examples_to_features(example_batch):
    input_encodings = tokenizer(example_batch["article"], max_length=1024, truncation=True)
    
    with tokenizer.as_target_tokenizer():
        target_encodings = tokenizer(example_batch["highlights"], max_length=256, truncation=True)
        
    return {"input_ids": input_encodings["input_ids"], 
           "attention_mask": input_encodings["attention_mask"], 
           "labels": target_encodings["input_ids"]}

dataset_tf = data.map(convert_examples_to_features, batched=True)

from transformers import DataCollatorForSeq2Seq
seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

from transformers import TrainingArguments, Trainer
training_args = TrainingArguments(output_dir='bart-multi-news', num_train_epochs=1, warmup_steps=500,                                  per_device_train_batch_size=1, per_device_eval_batch_size=1, weight_decay=0.01, logging_steps=10, push_to_hub=False, 
evaluation_strategy='steps', eval_steps=500, save_steps=1e6, 
gradient_accumulation_steps=16)

trainer = Trainer(model=model, args=training_args, tokenizer=tokenizer, 
                  data_collator=seq2seq_data_collator, 
                  train_dataset=dataset_tf["train"], 
                  eval_dataset=dataset_tf["validation"])

trainer.train()

