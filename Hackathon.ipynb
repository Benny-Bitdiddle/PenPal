{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertConfig, BertPreTrainedModel\n",
    "from torch.optim import AdamW\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sentencepiece\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/merrick/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"dataset/cnn_dailymail/train.csv\", nrows=1000)\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
       "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
       "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A drunk driver who killed a young woman in a h...</td>\n",
       "      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n",
       "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fleetwood are the only team still to have a 10...</td>\n",
       "      <td>Fleetwood top of League One after 2-0 win at S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
       "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
       "2  A drunk driver who killed a young woman in a h...   \n",
       "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
       "4  Fleetwood are the only team still to have a 10...   \n",
       "\n",
       "                                                   y  \n",
       "0  Bishop John Folda, of North Dakota, is taking ...  \n",
       "1  Criminal complaint: Cop used his role to help ...  \n",
       "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
       "3  Nina dos Santos says Europe must be ready to a...  \n",
       "4  Fleetwood top of League One after 2-0 win at S...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## keep the first N articles if you want to keep it lite \n",
    "dtf = pd.DataFrame(dataset).rename(columns={\"article\":\"text\", \n",
    "      \"highlights\":\"y\"})[[\"text\",\"y\"]]\n",
    "dtf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_preprocess_text(txt, punkt=True, lower=True, slang=True, lst_stopwords=None, stemm=False, lemm=True):\n",
    "    ### separate sentences with '. '\n",
    "    txt = re.sub(r'\\.(?=[^ \\W\\d])', '. ', str(txt))\n",
    "    ### remove punctuations and characters\n",
    "    txt = re.sub(r'[^\\w\\s]', '', txt) if punkt is True else txt\n",
    "    ### strip\n",
    "    txt = \" \".join([word.strip() for word in txt.split()])\n",
    "    ### lowercase\n",
    "    txt = txt.lower() if lower is True else txt\n",
    "    ### slang\n",
    "    txt = contractions.fix(txt) if slang is True else txt   \n",
    "    ### tokenize (convert from string to list)\n",
    "    lst_txt = txt.split()\n",
    "    ### stemming (remove -ing, -ly, ...)\n",
    "    if stemm is True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_txt = [ps.stem(word) for word in lst_txt]\n",
    "    ### lemmatization (convert the word into root word)\n",
    "    if lemm is True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_txt = [lem.lemmatize(word) for word in lst_txt]\n",
    "    ### remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_txt = [word for word in lst_txt if word not in \n",
    "                   lst_stopwords]\n",
    "    ### back to string\n",
    "    txt = \" \".join(lst_txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text\n",
    "dtf[\"text\"] = dtf[\"text\"].apply(lambda x: utils_preprocess_text(x))\n",
    "dtf[\"y\"] = dtf[\"y\"].apply(lambda x: utils_preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (918 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the text in the DataFrame\n",
    "dtf['text'] = dtf['text'].apply(lambda x: tokenizer.encode(x, return_tensors='pt'))\n",
    "\n",
    "# Tokenize the target summaries if applicable\n",
    "dtf['y'] = dtf['y'].apply(lambda x: tokenizer.encode(x, return_tensors='pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2011,  3378,  2811,  2405, 15471,  2487,  9765,  2423,  2255,\n",
      "          2286,  7172, 16710,  2575,  9765,  2423,  2255,  2286,  1996,  3387,\n",
      "          1997,  1996, 23054,  3234,  5801,  1999,  2167,  7734,  5292,  6086,\n",
      "          9280,  3634,  1997,  2277,  2266,  1999, 23054,  2882,  9292,  1998,\n",
      "         27435,  2000,  1996, 28389,  1037,  7865,  1999,  2397,  2244,  1998,\n",
      "          2220,  2255,  1996,  2110,  2740,  2533,  5292,  3843,  2019,  7319,\n",
      "          1997,  7524,  2005,  3087,  2040,  3230,  2274,  2277,  1998,  2165,\n",
      "         15661,  3387,  2198, 10671,  2050, 15885,  1997,  1996, 23054,  3234,\n",
      "          5801,  1999,  2167,  7734,  5292,  6086,  9280,  3634,  1997,  2277,\n",
      "          2266,  1999, 23054,  2882,  9292,  1998, 27435,  2000,  1996, 28389,\n",
      "          1037,  2110, 10047, 23041,  3989,  2565,  3208,  9618, 18473,  2360,\n",
      "          1996,  3891,  2003,  2659,  2021,  2880,  2514,  2009,  2590,  2000,\n",
      "          9499,  2111,  2000,  1996,  2825,  7524,  1996,  5801,  2623,  2006,\n",
      "          6928,  2008,  3387,  2198, 10671,  2050,  2003,  2635,  2051,  2125,\n",
      "          2044,  2108, 11441,  2007, 28389,  1037,  1996,  5801,  2360,  2002,\n",
      "         11016,  1996,  8985,  2083, 19450,  2833,  2096,  7052,  1037,  3034,\n",
      "          2005,  4397,  9492,  3387,  1999,  3304,  2197,  3204, 25353, 27718,\n",
      "          5358,  1997, 28389,  1037,  2421,  9016,  5458,  2791,  3279,  1997,\n",
      "         18923, 19029,  1998, 21419, 17964, 23054,  3234,  5801,  1999,  2167,\n",
      "          7734, 15885,  2003,  2073,  1996,  3387,  2003,  2284,   102]])\n"
     ]
    }
   ],
   "source": [
    "print(dtf[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Access the data using the 'text' key\n",
    "        return self.data[index]['text']\n",
    "\n",
    "# Construct the data loader with the same key 'text'\n",
    "train_dataset = CustomDataset(dtf)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'CustomDataset' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "# Example for configuring a pre-trained BERT model\n",
    "# Load pre-trained BERT model and configuration\n",
    "model_name = 'bert-base-uncased'\n",
    "config = BertConfig.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "# Modify the model configuration\n",
    "config.num_labels = 1  # Set number of labels for your specific task (e.g., binary classification)\n",
    "config.hidden_dropout_prob = 0.2  # Set dropout probability for hidden layers\n",
    "config.attention_probs_dropout_prob = 0.2  # Set dropout probability for attention layer\n",
    "\n",
    "# Create a custom architecture by subclassing BertPreTrainedModel\n",
    "class CustomBertModel(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(CustomBertModel, self).__init__(config)\n",
    "        # Define the layers of your custom model architecture here\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.linear = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Forward pass of your custom model\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]  # Use the pooled output for classification\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.linear(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Example for configuring model hyperparameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Load data, prepare data loader, and iterate through batches\n",
    "    for batch in train_dataloader:\n",
    "        # Zero gradients, forward pass, compute loss\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        # Compute loss and update weights\n",
    "        loss = compute_loss(logits, batch['labels'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# # Example for evaluating the model\n",
    "# # Load test data, prepare data loader\n",
    "# # Forward pass and compute evaluation metrics\n",
    "# for batch in test_data_loader:\n",
    "#     input_ids = batch['input_ids']\n",
    "#     attention_mask = batch['attention_mask']\n",
    "#     logits = model(input_ids, attention_mask)\n",
    "#     # Compute evaluation metrics (e.g., ROUGE, BLEU) using logits and ground truth summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
